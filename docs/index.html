<!DOCTYPE html>
<html lang="en">
<head lang="en">
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-FV0RC59Q5E"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-FV0RC59Q5E');
    </script>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>NeuMaDiff</title>

     <!-- mirror: F0%9F%AA%9E&lt -->
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/font-awesome.min.css">
    <link rel="stylesheet" href="css/app.css">
</head>

<body>
<div class="container" id="header" style="text-align: center; margin: auto;">
    <div class="row" id="title-row" style="max-width: 100%; margin: 0 auto; display: inline-block">
        <h2 class="col-md-12 text-center">
            <b>NeuMaDiff: Neural Material Synthesis via Hyperdiffusion</b><br>
            <small>
                NeurIPS'25 Workshop UniReps<br/>
                (Unifying Representations in Neural Models)
            </small>
        </h2>
    </div>
    <div class="row" id="author-row" style="margin:0 auto;">
        <!-- <div class="col-md-12 text-center" style="display: table; margin:0 auto"> -->
        <div class="text-center" style="display: table; margin:0 auto">
            <a href="https://chenliang-zhou.github.io" style="font-size: 18px;">Chenliang Zhou</a>,  <a href="https://peterhuistyping.github.io/" style="font-size: 18px;">Zheyuan Hu</a>,   <a href="https://asztr.github.io/" style="font-size: 18px;">Alejandro Sztrajman</a>, <a href="https://caiyancheng.github.io/academic.html" style="font-size: 18px;">Yancheng Cai</a>,  <a href="https://www.cst.cam.ac.uk/people/yl962" style="font-size: 18px;">Yaru Liu</a>,  <a href="https://www.cl.cam.ac.uk/~aco41/" style="font-size: 18px;">Cengiz Öztireli</a>. 
            <!-- <table class="author-table" id="author-table">
                <tr>
                    <td>
                        <a href="https://chenliang-zhou.github.io">Chenliang Zhou</a>
                    </td>
                    <td>
                        <a href="https://peterhuistyping.github.io/">Zheyuan Hu</a>
                    </td>
                    <td>
                        <a href="https://asztr.github.io/">Alejandro Sztrajman</a>
                    </td>
                </tr>
                <tr>    
                    <td>
                        <a href="https://caiyancheng.github.io/academic.html">Yancheng Cai</a>
                    </td>
                    <td>
                        <a href="https://www.cst.cam.ac.uk/people/yl962">Yaru Liu</a>
                    </td>
                    <td>
                        <a href="https://www.cl.cam.ac.uk/~aco41/">Cengiz Öztireli</a>
                    </td>
                </tr>
            </table> -->
            <br>
            <a href="https://www.cst.cam.ac.uk/ " style="font-size: 14px;">Department of Computer Science and Technology,</a>
            <br>
            <a href="https://www.cam.ac.uk/" style="font-size: 16px;">University of Cambridge</a>.
            <br><br>
        </div>
    </div>
</div>
<script>
    document.getElementById('author-row').style.maxWidth = document.getElementById("title-row").clientWidth + 'px';
</script>
<div class="container" id="main">
    <div class="row">
        <div class="col-md-4 col-md-offset-4 text-center">
            <ul class="nav nav-pills nav-justified">
                <li>
                    <a href="https://arxiv.org/abs/2411.12015">
                        <img src="./img/icon/paper_icon.png" alt="paper icon" height="70px">
                        <h4><strong>Paper</strong></h4>
                    </a>
                </li>
                <li>
                    <a href="https://github.com/PeterHUistyping/NeuMaDiff">
                        <img alt="github icon" src="./img/icon/github_icon.svg" height="70px">
                        <h4><strong>Code</strong></h4>
                    </a>
                </li>
                <li>
                    <a href="https://huggingface.co/datasets/Peter2023HuggingFace/NeuMERL">
                    <img src="./img/icon/data_icon.webp" height="90px">
                        <h4><strong>NeuMERL dataset (2.4k)</strong></h4>
                    </a>
                </li>
                <!-- <li>
                    <a href="">
                    <img src="./img/youtube_icon.png" height="90px">
                        <h4><strong>Video</strong></h4>
                    </a>
                </li> -->
            </ul>
        </div>

    </div>

    <img src="img/teaser.png" class="img-responsive" alt="TODO: main result" width="43%"
         style="max-height: 450px;margin:auto;"/>
    <div style="text-align: center;">
        <figcaption>3D models and scene rendered with our synthesized neural materials.</figcaption>
    </div>

    <div class="row">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                Abstract
            </h3>
            <p class="text-justify">
                High-quality material synthesis is essential for replicating complex surface properties to create realistic digital scenes. However, existing methods often suffer from inefficiencies in time and memory, require domain expertise, or demand extensive training data, with high-dimensional material data further constraining performance. Additionally, most approaches lack multi-modal guidance capabilities and standardized evaluation metrics, limiting control and comparability in synthesis tasks. <br/>
                To address these limitations, we propose <b>NeuMaDiff</b>, a novel <b>neu</b>ral <b>ma</b>terial synthesis framework utilizing hyper<b>diff</b>usion. Our method employs neural fields as a low-dimensional representation and incorporates a multi-modal conditional hyperdiffusion model to learn the distribution over material weights. This enables flexible guidance through inputs such as material type, text descriptions, or reference images, providing greater control over synthesis. <br/>
                 To support future research, we contribute two new material datasets and introduce two BRDF distributional metrics for more rigorous evaluation. We demonstrate the effectiveness of NeuMaDiff through extensive experiments, including a novel statistics-based constrained synthesis approach, which enables the generation of materials of desired categories. 
            </p>
        </div>
    </div>

    <div class="row">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                Dataset and base model
            </h3>
            Our NeuMERL dataset are uploaded at AI community Hugging Face <a href="https://huggingface.co/datasets/Peter2023HuggingFace/NeuMERL">NeuMERL dataset</a>, which contains 2400 BRDFs.
            <br/>
            For material synthesis, the weights of the pre-trained base models are uploaded at Hugging Face <a href="https://huggingface.co/Peter2023HuggingFace/NeuMaDiff">Synthesis model weights</a>.

            <h3>
                Material synthesis framework
            </h3>
            <figure>
                <div style="text-align: center;">
                    <img src="img/architecture.png" class="img-responsive" alt="synthesis framework" width="75%" />
                    <!--  style="max-height: 450px;margin:auto;" -->
                </div>
                <figcaption>
                    An overview of <b>NeuMaDiff</b>, our novel neural material
                    synthesis framework, consisting of three main stages. 
                    <!-- <br/> -->
                    1 (top left):
                    Data augmentation using RGB permutation and PCA interpolation
                    to create an expanded dataset, <i>AugMERL</i>; 
                    <!-- <br/> -->
                    2 (middle): Neural field
                    fitted to individual materials, resulting in <i>NeuMERL</i>, a dataset of
                    neural material representations;  
                    <!-- <br/> -->
                    3 (bottom): Training a multi-modal conditional hyperdiffusion model on NeuMERL to enable
                    conditional synthesis of high-quality, diverse materials guided by
                    inputs such as material type, text descriptions, or reference images.
                    <!-- <br/> -->
                    We further propose a novel statistics-based constrained synthesis
                    method to generate materials of a specified type (top right).
                </figcaption>
            </figure>
        </div>
    </div>

    <div class="row">
        <div class="col-md-8 col-md-offset-2">
            <h3>
                Novel metrics for material synthesis 
            </h3>
                The evaluation mainly focus on the <b>fidelity and diversity</b> of the synthesized materials, which is still an open problem in this field. To fill in the gap, we raise the idea of utilizing Minimum matching distance (MMD), Coverage (COV) and 1-nearest neighbor (1-NNA) with either the image quality metric or the BRDF distribution as underlying distance function.
                <br/>
                <br/>
                Please refer to <a href="https://arxiv.org/abs/2411.12015">our paper</a> about the details of the BRDF distributional metrics. 
                We demonstrate the effectiveness of NeuMaDiff through extensive experiments, including on this brand-new series of metrics.
            
            <h3>
                Unconditional synthesis evaluation
            </h3>

            <img src="img/uncond-vis.png" class="img-responsive" alt="uncond-vis" width="95%" />
            <!--  style="max-height: 450px;margin:auto;" -->
            <figcaption>
                Material synthesis across various pipelines. All baseline models fail to capture the underlying distribution effectively, resulting in
                meaningless samples with severe artefacts in the synthesized materials. In contrast, NeuMaDiff successfully captures the complex neural
                material distribution, achieving significantly better fidelity and diversity.
            </figcaption>

            <br/>

            <img src="img/unconditional-synthesis-metric-score.png" class="img-responsive" alt="unconditional-synthesis-metric-score" width="95%" />
            <!--  style="max-height: 450px;margin:auto;" -->
            <figcaption>
                Qualitative evaluation of unconditional synthesis with metrics assessing generation fidelity and diversity. ↓ indicates that a lower
                score is better and ↑ indicates the opposite. NeuMaDiff significantly outperforms all baseline models across these metrics, underscoring its
                effectiveness in neural material synthesis.
            </figcaption>
 
            <h3>
                Multi-modal synthesis evaluation
            </h3>
            We demonstrate the conditional synthesis capabilities of NeuMaDiff across various modalities of input: material type, text description, or material images.

            <br/>
            <br/>

            <img src="img/type-cond.png" class="img-responsive" alt="type-cond" width="45%" /> 
            <figcaption>
                <b>Type</b>-conditioned synthesis.  The synthesized materials
                are diverse and closely align with the specified material type.
            </figcaption>

            <br/>
            <br/>

            <img src="img/text-cond.png" class="img-responsive" alt="text-cond" width="45%" /> 
            <figcaption>
                <b>Text</b>-conditioned synthesis. NeuMaDiff synthesizes materials aligning with the texts and generalizes to <i>unseen</i> inputs:
                “green metal”, “red plastic”, and “highly specular material”.
            </figcaption>

            <br/>
            <br/>

            <img src="img/img-cond.png" class="img-responsive" alt="img-cond" width="45%" /> 
            <figcaption>
                <b>Image</b>-conditioned synthesis. In each of the six pairs,
                the left image is the conditioning input, while the right image is
                the synthesized result. NeuMaDiff effectively generates realistic
                materials that closely align with the conditioning images.
            </figcaption>
        

            <br/>
            <h3>
                Constrained synthesis evaluation
            </h3>
            We classify materials into seven categories based on
            their reflective properties: <i>diffuse, metallic, low-specular,
            medium-specular, high-specular, plastic, and mirror,</i> via a novel approach called constrained synthesis. 
            <br/>
            It complements our conditional
            pipeline by enforcing constraints on unconditionally synthesized samples, allowing for targeted material generation
            according to desired reflective characteristics. 
            Please refer to <a href="https://arxiv.org/abs/2411.12015">our paper</a> regarding the statistical constraints details.
            <br/>
            <br/>
            <img src="img/constrained-syn-vis.png" class="img-responsive" alt="constrained-syn-vis" width="85%" />
            <!--  style="max-height: 450px;margin:auto;" -->
            
            <figcaption>
                Synthesized materials of seven distinct categories using our novel constrained synthesis. Grounded in BRDF statistical analysis,
                this approach provides enhanced explainability and interpretability compared to standard conditional synthesis methods.
            </figcaption>


            
        <!-- <div class="row" id="bibtex"> -->
            <!-- <div class="col-md-8 col-md-offset-2"> -->
                <h3>
                    Citation  
                </h3>
                If you found the paper or code useful, please consider citing, <br/>
<pre><code>@misc{
    NeuMaDiff2024,
   title={NeuMaDiff: Neural Material Synthesis via Hyperdiffusion}, 
   author={Chenliang Zhou and Zheyuan Hu and Alejandro Sztrajman and Yancheng Cai and Yaru Liu and Cengiz Oztireli},
   year={2024},
   eprint={2411.12015},
   archivePrefix={arXiv},
   primaryClass={cs.GR},
   url={https://arxiv.org/abs/2411.12015}, 
}</code></pre>
            <!-- </div> -->
        <!-- </div> -->
            
        <br><br>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <p class="text-justify">
                    The website template was inspired by <a href="https://chenliang-zhou.github.io/FrePolad/">FrePolad</a>.
                </p>
            </div>
        </div>
    </div>
</div>

</body>
</html>
