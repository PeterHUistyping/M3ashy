{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "[optionally] create a virtual environment with Python 3.10.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy (from -r requirements.txt (line 1))\n",
      "  Downloading numpy-2.2.2-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting torch (from -r requirements.txt (line 2))\n",
      "  Downloading torch-2.5.1-cp312-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Collecting pandas (from -r requirements.txt (line 3))\n",
      "  Downloading pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Collecting tqdm (from -r requirements.txt (line 4))\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting scipy (from -r requirements.txt (line 5))\n",
      "  Downloading scipy-1.15.1-cp312-cp312-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Collecting scikit-learn (from -r requirements.txt (line 6))\n",
      "  Downloading scikit_learn-1.6.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Collecting opencv-python (from -r requirements.txt (line 7))\n",
      "  Using cached opencv_python-4.11.0.86-cp37-abi3-macosx_13_0_arm64.whl.metadata (20 kB)\n",
      "Collecting scikit-image (from -r requirements.txt (line 8))\n",
      "  Downloading scikit_image-0.25.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (14 kB)\n",
      "Collecting filelock (from torch->-r requirements.txt (line 2))\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch->-r requirements.txt (line 2))\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting networkx (from torch->-r requirements.txt (line 2))\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch->-r requirements.txt (line 2))\n",
      "  Using cached jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch->-r requirements.txt (line 2))\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting setuptools (from torch->-r requirements.txt (line 2))\n",
      "  Using cached setuptools-75.8.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting sympy==1.13.1 (from torch->-r requirements.txt (line 2))\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch->-r requirements.txt (line 2))\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas->-r requirements.txt (line 3))\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas->-r requirements.txt (line 3))\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->-r requirements.txt (line 3))\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->-r requirements.txt (line 6))\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->-r requirements.txt (line 6))\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pillow>=10.1 (from scikit-image->-r requirements.txt (line 8))\n",
      "  Downloading pillow-11.1.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.1 kB)\n",
      "Collecting imageio!=2.35.0,>=2.33 (from scikit-image->-r requirements.txt (line 8))\n",
      "  Using cached imageio-2.37.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image->-r requirements.txt (line 8))\n",
      "  Using cached tifffile-2025.1.10-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting packaging>=21 (from scikit-image->-r requirements.txt (line 8))\n",
      "  Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image->-r requirements.txt (line 8))\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 3))\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch->-r requirements.txt (line 2))\n",
      "  Downloading MarkupSafe-3.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Downloading numpy-2.2.2-cp312-cp312-macosx_14_0_arm64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.5.1-cp312-none-macosx_11_0_arm64.whl (63.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hUsing cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Downloading pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl (11.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading scipy-1.15.1-cp312-cp312-macosx_14_0_arm64.whl (24.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.9/24.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.6.1-cp312-cp312-macosx_12_0_arm64.whl (11.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached opencv_python-4.11.0.86-cp37-abi3-macosx_13_0_arm64.whl (37.3 MB)\n",
      "Downloading scikit_image-0.25.0-cp312-cp312-macosx_12_0_arm64.whl (13.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached imageio-2.37.0-py3-none-any.whl (315 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Downloading pillow-11.1.0-cp312-cp312-macosx_11_0_arm64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached tifffile-2025.1.10-py3-none-any.whl (227 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Using cached jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
      "Using cached setuptools-75.8.0-py3-none-any.whl (1.2 MB)\n",
      "Downloading MarkupSafe-3.0.2-cp312-cp312-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: pytz, mpmath, tzdata, typing-extensions, tqdm, threadpoolctl, sympy, six, setuptools, pillow, packaging, numpy, networkx, MarkupSafe, joblib, fsspec, filelock, tifffile, scipy, python-dateutil, opencv-python, lazy-loader, jinja2, imageio, torch, scikit-learn, scikit-image, pandas\n",
      "Successfully installed MarkupSafe-3.0.2 filelock-3.16.1 fsspec-2024.12.0 imageio-2.37.0 jinja2-3.1.5 joblib-1.4.2 lazy-loader-0.4 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.2 opencv-python-4.11.0.86 packaging-24.2 pandas-2.2.3 pillow-11.1.0 python-dateutil-2.9.0.post0 pytz-2024.2 scikit-image-0.25.0 scikit-learn-1.6.1 scipy-1.15.1 setuptools-75.8.0 six-1.17.0 sympy-1.13.1 threadpoolctl-3.5.0 tifffile-2025.1.10 torch-2.5.1 tqdm-4.67.1 typing-extensions-4.12.2 tzdata-2024.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the dataset and model  \n",
    "Please refer to the instructions in [README.md](./README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeuMaDiff: Neural Material Synthesis via Hyperdiffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output folder\n",
    "!mkdir -p output/generation/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sample synthetic materials from the pre-trained hyperdiffusion [synthesis model weights](https://huggingface.co/Peter2023HuggingFace/M3ashy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using following input parameter splits: [675, 257]\n",
      "['mlp_weights', 'timestep_embedding']\n",
      "number of parameters: 86,293,248\n",
      "{\n",
      "    \"model\": \"HyperDiffusion\",\n",
      "    \"use_cond\": false,\n",
      "    \"cond_label\": {\n",
      "        \"unconditional_guidance_scale\": 0\n",
      "    }\n",
      "}\n",
      "(2400, 675)\n",
      "Train/Infer split with seed value: 0\n",
      "Train size: 2280  Infer size: 120\n",
      "/Users/peterhu/Documents/Coding/GitHub/Paper/NeuMaDiff/src/pytorch/model_factory.py:175: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(path, map_location=self.device))\n",
      "Inference dataset: MERL\n",
      "Inference: Unconditional size: 100\n",
      "tensor([[-0.9924,  0.0108, -0.7927,  ...,  0.0727,  0.3032,  1.2927],\n",
      "        [-0.5722,  0.0135, -1.4341,  ...,  1.8653,  0.9280,  0.6289],\n",
      "        [-0.9511,  0.0139, -0.1536,  ...,  0.8391,  0.3050,  0.7217],\n",
      "        ...,\n",
      "        [-1.3078,  0.0152, -0.7911,  ...,  1.3815,  0.8492,  0.7501],\n",
      "        [-1.2725,  0.0143, -0.2768,  ...,  1.4630,  0.9539,  1.1509],\n",
      "        [-1.5562,  0.0109, -0.4152,  ...,  1.1256,  0.9057,  0.8271]])\n"
     ]
    }
   ],
   "source": [
    "!python src/pytorch/train.py --file_index -1  --pytorch_model_type 2 --sample 1 --model_weights_path model/NeuMaDiff-diversity.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using following input parameter splits: [675, 257]\n",
      "['mlp_weights', 'timestep_embedding']\n",
      "number of parameters: 86,293,248\n",
      "{\n",
      "    \"model\": \"HyperDiffusion\",\n",
      "    \"use_cond\": false,\n",
      "    \"cond_label\": {\n",
      "        \"unconditional_guidance_scale\": 0\n",
      "    }\n",
      "}\n",
      "(2400, 675)\n",
      "Train/Infer split with seed value: 0\n",
      "Train size: 2280  Infer size: 120\n",
      "/Users/peterhu/Documents/Coding/GitHub/Paper/NeuMaDiff/src/pytorch/model_factory.py:175: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(path, map_location=self.device))\n",
      "Inference dataset: MERL\n",
      "Inference: Unconditional size: 100\n",
      "tensor([[-1.6848,  0.0162, -0.4454,  ...,  0.5941,  0.6153,  0.9915],\n",
      "        [-1.1722,  0.0152, -0.2063,  ...,  1.4140,  1.1715,  1.4054],\n",
      "        [-0.0486,  0.0156, -0.2233,  ...,  0.9633,  1.0735,  1.2543],\n",
      "        ...,\n",
      "        [-0.9515,  0.0158, -0.7861,  ...,  0.4209,  0.6796, -0.0901],\n",
      "        [-1.3269,  0.0162, -1.0671,  ...,  1.1435,  0.9456,  1.2468],\n",
      "        [-0.9992,  0.0158, -0.8038,  ...,  0.3063,  1.2300,  0.3743]])\n"
     ]
    }
   ],
   "source": [
    "!python src/pytorch/train.py --file_index -1  --pytorch_model_type 2 --sample 1 --model_weights_path model/NeuMaDiff-quality.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Create folders for the generated materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p output/generation/mlp/mlp_gen{0..120}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extract the MLP model from the npy file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model: output/generation/mlp/mlp_gen0/model0.pth\n",
      "Saved model: output/generation/mlp/mlp_gen1/model1.pth\n",
      "Saved model: output/generation/mlp/mlp_gen2/model2.pth\n",
      "Saved model: output/generation/mlp/mlp_gen3/model3.pth\n",
      "Saved model: output/generation/mlp/mlp_gen4/model4.pth\n",
      "Saved model: output/generation/mlp/mlp_gen5/model5.pth\n",
      "Saved model: output/generation/mlp/mlp_gen6/model6.pth\n",
      "Saved model: output/generation/mlp/mlp_gen7/model7.pth\n",
      "Saved model: output/generation/mlp/mlp_gen8/model8.pth\n",
      "Saved model: output/generation/mlp/mlp_gen9/model9.pth\n",
      "Saved model: output/generation/mlp/mlp_gen10/model10.pth\n",
      "Saved model: output/generation/mlp/mlp_gen11/model11.pth\n",
      "Saved model: output/generation/mlp/mlp_gen12/model12.pth\n",
      "Saved model: output/generation/mlp/mlp_gen13/model13.pth\n",
      "Saved model: output/generation/mlp/mlp_gen14/model14.pth\n",
      "Saved model: output/generation/mlp/mlp_gen15/model15.pth\n",
      "Saved model: output/generation/mlp/mlp_gen16/model16.pth\n",
      "Saved model: output/generation/mlp/mlp_gen17/model17.pth\n",
      "Saved model: output/generation/mlp/mlp_gen18/model18.pth\n",
      "Saved model: output/generation/mlp/mlp_gen19/model19.pth\n",
      "Saved model: output/generation/mlp/mlp_gen20/model20.pth\n",
      "Saved model: output/generation/mlp/mlp_gen21/model21.pth\n",
      "Saved model: output/generation/mlp/mlp_gen22/model22.pth\n",
      "Saved model: output/generation/mlp/mlp_gen23/model23.pth\n",
      "Saved model: output/generation/mlp/mlp_gen24/model24.pth\n",
      "Saved model: output/generation/mlp/mlp_gen25/model25.pth\n",
      "Saved model: output/generation/mlp/mlp_gen26/model26.pth\n",
      "Saved model: output/generation/mlp/mlp_gen27/model27.pth\n",
      "Saved model: output/generation/mlp/mlp_gen28/model28.pth\n",
      "Saved model: output/generation/mlp/mlp_gen29/model29.pth\n",
      "Saved model: output/generation/mlp/mlp_gen30/model30.pth\n",
      "Saved model: output/generation/mlp/mlp_gen31/model31.pth\n",
      "Saved model: output/generation/mlp/mlp_gen32/model32.pth\n",
      "Saved model: output/generation/mlp/mlp_gen33/model33.pth\n",
      "Saved model: output/generation/mlp/mlp_gen34/model34.pth\n",
      "Saved model: output/generation/mlp/mlp_gen35/model35.pth\n",
      "Saved model: output/generation/mlp/mlp_gen36/model36.pth\n",
      "Saved model: output/generation/mlp/mlp_gen37/model37.pth\n",
      "Saved model: output/generation/mlp/mlp_gen38/model38.pth\n",
      "Saved model: output/generation/mlp/mlp_gen39/model39.pth\n",
      "Saved model: output/generation/mlp/mlp_gen40/model40.pth\n",
      "Saved model: output/generation/mlp/mlp_gen41/model41.pth\n",
      "Saved model: output/generation/mlp/mlp_gen42/model42.pth\n",
      "Saved model: output/generation/mlp/mlp_gen43/model43.pth\n",
      "Saved model: output/generation/mlp/mlp_gen44/model44.pth\n",
      "Saved model: output/generation/mlp/mlp_gen45/model45.pth\n",
      "Saved model: output/generation/mlp/mlp_gen46/model46.pth\n",
      "Saved model: output/generation/mlp/mlp_gen47/model47.pth\n",
      "Saved model: output/generation/mlp/mlp_gen48/model48.pth\n",
      "Saved model: output/generation/mlp/mlp_gen49/model49.pth\n",
      "Saved model: output/generation/mlp/mlp_gen50/model50.pth\n",
      "Saved model: output/generation/mlp/mlp_gen51/model51.pth\n",
      "Saved model: output/generation/mlp/mlp_gen52/model52.pth\n",
      "Saved model: output/generation/mlp/mlp_gen53/model53.pth\n",
      "Saved model: output/generation/mlp/mlp_gen54/model54.pth\n",
      "Saved model: output/generation/mlp/mlp_gen55/model55.pth\n",
      "Saved model: output/generation/mlp/mlp_gen56/model56.pth\n",
      "Saved model: output/generation/mlp/mlp_gen57/model57.pth\n",
      "Saved model: output/generation/mlp/mlp_gen58/model58.pth\n",
      "Saved model: output/generation/mlp/mlp_gen59/model59.pth\n",
      "Saved model: output/generation/mlp/mlp_gen60/model60.pth\n",
      "Saved model: output/generation/mlp/mlp_gen61/model61.pth\n",
      "Saved model: output/generation/mlp/mlp_gen62/model62.pth\n",
      "Saved model: output/generation/mlp/mlp_gen63/model63.pth\n",
      "Saved model: output/generation/mlp/mlp_gen64/model64.pth\n",
      "Saved model: output/generation/mlp/mlp_gen65/model65.pth\n",
      "Saved model: output/generation/mlp/mlp_gen66/model66.pth\n",
      "Saved model: output/generation/mlp/mlp_gen67/model67.pth\n",
      "Saved model: output/generation/mlp/mlp_gen68/model68.pth\n",
      "Saved model: output/generation/mlp/mlp_gen69/model69.pth\n",
      "Saved model: output/generation/mlp/mlp_gen70/model70.pth\n",
      "Saved model: output/generation/mlp/mlp_gen71/model71.pth\n",
      "Saved model: output/generation/mlp/mlp_gen72/model72.pth\n",
      "Saved model: output/generation/mlp/mlp_gen73/model73.pth\n",
      "Saved model: output/generation/mlp/mlp_gen74/model74.pth\n",
      "Saved model: output/generation/mlp/mlp_gen75/model75.pth\n",
      "Saved model: output/generation/mlp/mlp_gen76/model76.pth\n",
      "Saved model: output/generation/mlp/mlp_gen77/model77.pth\n",
      "Saved model: output/generation/mlp/mlp_gen78/model78.pth\n",
      "Saved model: output/generation/mlp/mlp_gen79/model79.pth\n",
      "Saved model: output/generation/mlp/mlp_gen80/model80.pth\n",
      "Saved model: output/generation/mlp/mlp_gen81/model81.pth\n",
      "Saved model: output/generation/mlp/mlp_gen82/model82.pth\n",
      "Saved model: output/generation/mlp/mlp_gen83/model83.pth\n",
      "Saved model: output/generation/mlp/mlp_gen84/model84.pth\n",
      "Saved model: output/generation/mlp/mlp_gen85/model85.pth\n",
      "Saved model: output/generation/mlp/mlp_gen86/model86.pth\n",
      "Saved model: output/generation/mlp/mlp_gen87/model87.pth\n",
      "Saved model: output/generation/mlp/mlp_gen88/model88.pth\n",
      "Saved model: output/generation/mlp/mlp_gen89/model89.pth\n",
      "Saved model: output/generation/mlp/mlp_gen90/model90.pth\n",
      "Saved model: output/generation/mlp/mlp_gen91/model91.pth\n",
      "Saved model: output/generation/mlp/mlp_gen92/model92.pth\n",
      "Saved model: output/generation/mlp/mlp_gen93/model93.pth\n",
      "Saved model: output/generation/mlp/mlp_gen94/model94.pth\n",
      "Saved model: output/generation/mlp/mlp_gen95/model95.pth\n",
      "Saved model: output/generation/mlp/mlp_gen96/model96.pth\n",
      "Saved model: output/generation/mlp/mlp_gen97/model97.pth\n",
      "Saved model: output/generation/mlp/mlp_gen98/model98.pth\n",
      "Saved model: output/generation/mlp/mlp_gen99/model99.pth\n"
     ]
    }
   ],
   "source": [
    "!python src/tools/merl_workflow/read_mlp_weight.py --file_index -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Infer the binary files of the synthesized materials from the MLP model, following [MERL](https://www.merl.com/research/downloads/BRDF) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n",
      "(4374000,)\n"
     ]
    }
   ],
   "source": [
    "!python src/tools/merl_workflow/write_merl_binary.py --file_index -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rendering with the binary material to images.\n",
    "\n",
    "We use [Mitsuba](https://www.mitsuba-renderer.org/), a physically based renderer, to render the 3D models with the synthesized materials.  You may find [Neural-BRDF](https://github.com/asztr/Neural-BRDF) helpful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Optionally] Train a new hyperdiffusion model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using following input parameter splits: [675, 257]\n",
      "['mlp_weights', 'timestep_embedding']\n",
      "number of parameters: 86,293,248\n",
      "{\n",
      "    \"model\": \"HyperDiffusion\",\n",
      "    \"use_cond\": false,\n",
      "    \"cond_label\": {\n",
      "        \"unconditional_guidance_scale\": 0\n",
      "    }\n",
      "}\n",
      "(2400, 675)\n",
      "Train/Infer split with seed value: 0\n",
      "Train size: 2280  Infer size: 120\n",
      "Batch size:  16\n",
      "Number of epochs:  1\n",
      "Train epoch:1\n",
      "Epoch number [1/1], train Loss:1.2302\n",
      "Lr:5.00E-04\n"
     ]
    }
   ],
   "source": [
    "!python src/pytorch/train.py --file_index -1  --pytorch_model_type 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of synthesized materials\n",
    "\n",
    "Please update the folder and filename to `.binary` files or render images of reference and synthesized sets.\n",
    "\n",
    "There are two underlying distance metrics: BRDF space and image space. (For BRDF space, the demo use `data/merl/blue-metallic-paint.binary` from MERL dataset.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For BRDF space, the demo use `data/merl/blue-metallic-paint.binary` from MERL dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reference BRDF: 100%|█████████████████████| 1/1 [00:00<00:00,  5.95it/s]\n",
      "Loading sample BRDF: 100%|████████████████████████| 1/1 [00:00<00:00,  6.06it/s]\n",
      "Type: 1\n",
      "Metrics: Log-cos-L1\n",
      "Pairwise Distance: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.17it/s]\n",
      "Matched reference BRDF id indexed by sample BRDF:\n",
      "tensor([0], device='mps:0')\n",
      "Distance: 100%|███████████████████████████████████| 1/1 [00:00<00:00, 12.95it/s]\n",
      "Distance: 100%|███████████████████████████████████| 1/1 [00:00<00:00, 14.56it/s]\n",
      "[minimum matching distance (from ref) (↓)]: 0.00000000\n",
      "[coverage (↑)]: 1.00000000\n",
      "[minimum matching distance (from sample)]: 0.00000000\n",
      "[1-Nearest-Neighbour-acc_t (50%)]: 0.00000000\n",
      "[1-Nearest-Neighbour-acc_f (50%)]: 0.00000000\n",
      "[1-Nearest-Neighbour-acc (50%)]: 0.00000000\n",
      "Type: 2\n",
      "Metrics: Log-L1\n",
      "Pairwise Distance: 100%|██████████████████████████| 1/1 [00:00<00:00, 24.12it/s]\n",
      "Matched reference BRDF id indexed by sample BRDF:\n",
      "tensor([0], device='mps:0')\n",
      "Distance: 100%|███████████████████████████████████| 1/1 [00:00<00:00, 29.86it/s]\n",
      "Distance: 100%|███████████████████████████████████| 1/1 [00:00<00:00, 29.65it/s]\n",
      "[minimum matching distance (from ref) (↓)]: 0.00000000\n",
      "[coverage (↑)]: 1.00000000\n",
      "[minimum matching distance (from sample)]: 0.00000000\n",
      "[1-Nearest-Neighbour-acc_t (50%)]: 0.00000000\n",
      "[1-Nearest-Neighbour-acc_f (50%)]: 0.00000000\n",
      "[1-Nearest-Neighbour-acc (50%)]: 0.00000000\n",
      "Type: 3\n",
      "Metrics: L1\n",
      "Pairwise Distance: 100%|██████████████████████████| 1/1 [00:00<00:00, 31.46it/s]\n",
      "Matched reference BRDF id indexed by sample BRDF:\n",
      "tensor([0], device='mps:0')\n",
      "Distance: 100%|███████████████████████████████████| 1/1 [00:00<00:00, 30.97it/s]\n",
      "Distance: 100%|███████████████████████████████████| 1/1 [00:00<00:00, 31.40it/s]\n",
      "[minimum matching distance (from ref) (↓)]: 0.00000000\n",
      "[coverage (↑)]: 1.00000000\n",
      "[minimum matching distance (from sample)]: 0.00000000\n",
      "[1-Nearest-Neighbour-acc_t (50%)]: 0.00000000\n",
      "[1-Nearest-Neighbour-acc_f (50%)]: 0.00000000\n",
      "[1-Nearest-Neighbour-acc (50%)]: 0.00000000\n"
     ]
    }
   ],
   "source": [
    "# BRDF space\n",
    "!python src/eval/metrics.py --is_brdf_space 1 --refer_set_size 1  --reference_folder_path \"data/merl/\" --sample_set_size 1  --sample_folder_path \"data/merl/\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For BRDF space, the demo use `data/merl/blue-metallic-paint.binary` from MERL dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reference BRDF images: 100%|█████████████| 1/1 [00:00<00:00, 166.96it/s]\n",
      "(1, 256, 256, 3)\n",
      "Loading sample BRDF images: 100%|███████████████| 1/1 [00:00<00:00, 1102.89it/s]\n",
      "(1, 256, 256, 3)\n",
      "Type: 1\n",
      "Metrics: SSIM\n",
      "Pairwise Distance: 100%|██████████████████████████| 1/1 [00:00<00:00, 53.20it/s]\n",
      "Matched reference BRDF id indexed by sample BRDF:\n",
      "tensor([0])\n",
      "Distance: 100%|██████████████████████████████████| 1/1 [00:00<00:00, 468.38it/s]\n",
      "Distance: 100%|██████████████████████████████████| 1/1 [00:00<00:00, 456.65it/s]\n",
      "[minimum matching distance (from ref) (↓)]: -0.01197730\n",
      "[coverage (↑)]: 1.00000000\n",
      "[minimum matching distance (from sample)]: -0.01197730\n",
      "[1-Nearest-Neighbour-acc_t (50%)]: 0.00000000\n",
      "[1-Nearest-Neighbour-acc_f (50%)]: 0.00000000\n",
      "[1-Nearest-Neighbour-acc (50%)]: 0.00000000\n",
      "Type: 2\n",
      "Metrics: PSNR\n",
      "Pairwise Distance: 100%|██████████████████████████| 1/1 [00:00<00:00, 60.07it/s]\n",
      "Matched reference BRDF id indexed by sample BRDF:\n",
      "tensor([0])\n",
      "Distance: 100%|██████████████████████████████████| 1/1 [00:00<00:00, 431.78it/s]\n",
      "Distance: 100%|██████████████████████████████████| 1/1 [00:00<00:00, 398.70it/s]\n",
      "[minimum matching distance (from ref) (↓)]: -7.72977437\n",
      "[coverage (↑)]: 1.00000000\n",
      "[minimum matching distance (from sample)]: -7.72977437\n",
      "[1-Nearest-Neighbour-acc_t (50%)]: 0.00000000\n",
      "[1-Nearest-Neighbour-acc_f (50%)]: 0.00000000\n",
      "[1-Nearest-Neighbour-acc (50%)]: 0.00000000\n",
      "Type: 3\n",
      "Metrics: NRMSE\n",
      "Pairwise Distance: 100%|██████████████████████████| 1/1 [00:00<00:00, 58.07it/s]\n",
      "Matched reference BRDF id indexed by sample BRDF:\n",
      "tensor([0])\n",
      "Distance: 100%|██████████████████████████████████| 1/1 [00:00<00:00, 427.12it/s]\n",
      "Distance: 100%|██████████████████████████████████| 1/1 [00:00<00:00, 484.27it/s]\n",
      "[minimum matching distance (from ref) (↓)]: 0.78848776\n",
      "[coverage (↑)]: 1.00000000\n",
      "[minimum matching distance (from sample)]: 0.78848776\n",
      "[1-Nearest-Neighbour-acc_t (50%)]: 0.00000000\n",
      "[1-Nearest-Neighbour-acc_f (50%)]: 0.00000000\n",
      "[1-Nearest-Neighbour-acc (50%)]: 0.00000000\n",
      "Type: 4\n",
      "Metrics: MSE\n",
      "Pairwise Distance: 100%|██████████████████████████| 1/1 [00:00<00:00, 55.60it/s]\n",
      "Matched reference BRDF id indexed by sample BRDF:\n",
      "tensor([0])\n",
      "Distance: 100%|██████████████████████████████████| 1/1 [00:00<00:00, 500.45it/s]\n",
      "Distance: 100%|██████████████████████████████████| 1/1 [00:00<00:00, 447.25it/s]\n",
      "[minimum matching distance (from ref) (↓)]: 10967.38081360\n",
      "[coverage (↑)]: 1.00000000\n",
      "[minimum matching distance (from sample)]: 10967.38081360\n",
      "[1-Nearest-Neighbour-acc_t (50%)]: 0.00000000\n",
      "[1-Nearest-Neighbour-acc_f (50%)]: 0.00000000\n",
      "[1-Nearest-Neighbour-acc (50%)]: 0.00000000\n"
     ]
    }
   ],
   "source": [
    "# image space\n",
    "!python src/eval/metrics.py --is_brdf_space 0 --refer_set_size 1 --reference_img_path \"output/img/\" --sample_set_size 1 --sample_img_path \"output/img/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Optional] NeuMERL: Training MLP from scratch\n",
    "\n",
    "To train the NeuMERL from scratch, please download MERL dataset from [MERL](https://www.merl.com/research/downloads/BRDF) and put the binary files in the `data/merl` folder (see details [here](./data/merl/README.md)). Please download the [initial model weights](https://huggingface.co/datasets/Peter2023HuggingFace/NeuMERL/blob/main/mlp_weights_ini.pth) and put them in the `model` folder (see details [here](model/README.md))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output folder\n",
    "!mkdir -p output/merl/merl_1/blue-metallic-paint/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train a NeuMERL MLP model from scratch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 blue-metallic-paint\n",
      "Batch size:  512\n",
      "Number of epochs:  1\n",
      "/Users/peterhu/Documents/Coding/GitHub/Paper/NeuMaDiff/src/pytorch/model_factory.py:175: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(path, map_location=self.device))\n",
      "Using the same initial weights for MLP\n",
      "Train epoch:1\n",
      "Epoch number [1/1], train Loss:0.0866\n",
      "Epoch [1/1], val loss:0.0656\n",
      "Training 1 blue-metallic-paint finished\n"
     ]
    }
   ],
   "source": [
    "!python src/pytorch/train.py --pytorch_model_type 1  --file_index 1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See more details in the [README.md](./README.md)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
